from typing import List
from llama_index.core import Document, SimpleDirectoryReader
from llama_index.core.extractors import SummaryExtractor, QuestionsAnsweredExtractor
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.node_parser import (
    SentenceSplitter,
)
from llama_index.core import VectorStoreIndex, StorageContext

from llama_index.core.storage.docstore import SimpleDocumentStore
from llama_index.core.storage.index_store import SimpleIndexStore

from src.config import app_settings, service_factory

DEFAULT_SUMMARY_EXTRACT_TEMPLATE = """
–û—Å—å –∑–º—ñ—Å—Ç —Ä–æ–∑–¥—ñ–ª—É:
{context_str}

–ü—ñ–¥—Å—É–º—É–π—Ç–µ –æ—Å–Ω–æ–≤–Ω—ñ —Ç–µ–º–∏ —Ç–∞ –ø–æ–Ω—è—Ç—Ç—è —Ä–æ–∑–¥—ñ–ª—É. 

–ü—ñ–¥—Å—É–º–æ–∫: """


DEFAULT_QUESTION_GEN_TMPL = """\
–û—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç:
{context_str}

–ó –æ–≥–ª—è–¥—É –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, \
—Å—Ç–≤–æ—Ä—ñ—Ç—å {num_questions} –ø–∏—Ç–∞–Ω—å, –Ω–∞ —è–∫—ñ —Ü–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–∂–µ –Ω–∞–¥–∞—Ç–∏ \
–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ, —è–∫—ñ –Ω–∞–≤—Ä—è–¥ —á–∏ –º–æ–∂–Ω–∞ –∑–Ω–∞–π—Ç–∏ –¥–µ—ñ–Ω–¥–µ.

–¢–∞–∫–æ–∂ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –Ω–∞–¥–∞–Ω—ñ –±—ñ–ª—å—à –∑–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∑—é–º–µ –Ω–∞–≤–∫–æ–ª–∏—à–Ω—å–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. \
–°–ø—Ä–æ–±—É–π—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ü—ñ —Ä–µ–∑—é–º–µ, —â–æ–± —Å—Ç–≤–æ—Ä–∏—Ç–∏ –∫—Ä–∞—â—ñ –ø–∏—Ç–∞–Ω–Ω—è, \
–Ω–∞ —è–∫—ñ —Ü–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–∂–µ –≤—ñ–¥–ø–æ–≤—ñ—Å—Ç–∏.

"""

class DocumentIndexer:

    def __init__(self):
        self._create_pipeline()

    def _create_pipeline(self):
        transformations = [
            SentenceSplitter(
                chunk_size=app_settings.chunk_size,
                chunk_overlap=app_settings.chunk_overlap
            ),
            SummaryExtractor(prompt_template=DEFAULT_SUMMARY_EXTRACT_TEMPLATE),
            QuestionsAnsweredExtractor(prompt_template=DEFAULT_SUMMARY_EXTRACT_TEMPLATE, ),
            service_factory.embedding_model
        ]

        self.pipeline = IngestionPipeline(
            transformations=transformations,
            vector_store=service_factory.vectorstore.get_vector_store()
        )

    def load_documents(self) -> List[Document]:
        print(f"üìö –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ {app_settings.data_dir}...")

        reader = SimpleDirectoryReader(
            input_dir=app_settings.data_dir,
            recursive=True,
            required_exts=[".pdf", ".txt", ".md", ".docx"]
        )

        documents = reader.load_data()
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")

        return documents

    def index_documents(self):
        documents = self.load_documents()

        print("üîÑ –û–±—Ä–æ–±–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ IngestionPipeline...")

        nodes = self.pipeline.run(
            documents=documents,
            show_progress=True
        )

        vector_store = service_factory.vectorstore.get_vector_store()

        docstore = SimpleDocumentStore()
        index_store = SimpleIndexStore()

        storage_context = StorageContext.from_defaults(
            docstore=docstore,
            index_store=index_store,
            vector_store=vector_store,
            persist_dir=app_settings.persist_dir,
        )

        storage_context.docstore.add_documents(nodes)

        index = VectorStoreIndex(
            nodes=nodes,
            storage_context=storage_context,
            embed_model=service_factory.embedding_model,
            show_progress=False,
            store_nodes_override=True
        )

        index.storage_context.persist()

        print("\n" + "=" * 60)
        print("‚úÖ –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
        print("=" * 60 + "\n")
        return len(nodes)
